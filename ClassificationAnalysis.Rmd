---
title: "Classfication Analysis"
author: "Mohammed Ali"
date: "January 24, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(data.table) #reading table
library(dplyr)
library(class)
library(naivebayes)
library(pROC)
library(rpart)
library(rpart.plot)
library(randomForest)
```

# k-Nearest Neighbors (kNN)

As the kNN algorithm literally "learns by example" it is a case in point for starting to understand supervised machine learning. This chapter will introduce classification while working through the application of kNN to self-driving vehicle road sign recognition.

## Recognizing a road sign with kNN
After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone.

```{r load_data_2}
signs <- fread("data/knn_traffic_signs.csv", stringsAsFactors = FALSE,
               showProgress = FALSE)
train_signs <- signs[sample == "train"]
test_signs <- signs[sample == "test"]
glimpse(signs)
```

Create a vector of sign labels to use with kNN by extracting the column 
```{r sign_types}
# Create a vector of labels
sign_types <- train_signs$sign_type
```

Identify the next_sign using the `knn()` function:

* Set the `train` argument equal to the `signs` data frame without the first column.
* Set the `test` argument equal to the data frame `next_sign`.
* Use the vector of labels `cl` argument.

```{r first_knn_classifier}
# Classify the next sign observed
knn(train = train_signs[, -c("sign_type", "id", "sample")], test = test_signs[1:1, -c("sign_type", "id", "sample")], cl = sign_types)
```


## Exploring the traffic sign dataset
To better understand how the `knn()` function was able to classify the stop sign, it may help to examine the training dataset it used.

Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center.

```{r count_sign_types}
# Count the number of signs of each type
table(train_signs$sign_type)
```

Run the provided `aggregate()` command to see whether the average red level might vary by sign type.
```{r aggregate}
# Check r10's average red level by sign type
aggregate(r10 ~ sign_type, data = train_signs, mean)
```


## Classifying a collection of road signs
Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course.

```{r predict_all}
# Use kNN to identify the test road signs
signs_pred <- knn(train = train_signs[, -c("sign_type", "id", "sample")], test = test_signs[, -c("sign_type", "id", "sample")], cl = sign_types)


# Create a confusion matrix of the actual versus predicted values
signs_actual <- test_signs$sign_type
table(signs_pred, signs_actual)

# Compute the accuracy
mean(signs_pred == signs_actual)
```

## Testing other 'k' values

By default, the `knn()` function in the class package uses only the single nearest neighbor.

Setting a `k` parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class.

Compare `k` values of _1_, _7_, and _15_ to examine the impact on traffic sign classification accuracy.

```{r different_ks}
# Compute the accuracy of the baseline model (default k = 1)
k_1 <- knn(train = train_signs[, -c("sign_type", "id", "sample")], test = test_signs[, -c("sign_type", "id", "sample")], cl = sign_types)
mean(signs_actual == k_1)

# Modify the above to set k = 7
k_7 <- knn(train = train_signs[, -c("sign_type", "id", "sample")], test = test_signs[, -c("sign_type", "id", "sample")], cl = sign_types, k = 7)
mean(signs_actual == k_7)

# Set k = 15 and compare to the above
k_15 <- knn(train = train_signs[, -c("sign_type", "id", "sample")], test = test_signs[, -c("sign_type", "id", "sample")], cl = sign_types, k = 15)
mean(signs_actual == k_15)
```

## Seeing how the neighbors voted
When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated.

For example, knowing more about the voters' confidence in the classification could allow an autonomous vehicle to use caution in the case there is any chance at all that a stop sign is ahead.

Build a kNN model with the `prob = TRUE` parameter to compute the vote proportions. Set `k = 7`.
```{r prob}
# Use the prob parameter to get the proportion of votes for the winning class
sign_pred <- knn(train = train_signs[, -c("sign_type", "id", "sample")], test = test_signs[, -c("sign_type", "id", "sample")], cl = sign_types, k = 7, prob = TRUE)

# Get the "prob" attribute from the predicted classes
sign_prob <- attr(sign_pred, "prob")

# Examine the first several predictions
head(sign_pred)

# Examine the proportion of votes for the winning class
head(sign_prob)
```




# Naive Bayes
Naive Bayes uses principles from the field of statistics to make predictions. This chapter will introduce the basics of Bayesian methods while exploring how to apply these techniques to iPhone-like destination suggestions.

The `where9am` data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his `location` at 9am each day as well as whether the `daytype` was a weekend or weekday.


Using the conditional probability formula below, you can compute the probability that Brett is working in the office, given that it is a weekday.

$P(A|B)=P(A and B)/P(B)$

Calculations like these are the basis of the Naive Bayes destination prediction model you'll develop in later exercises.

Loading data

```{r load_data}
where9am <- fread("data/locations.csv", stringsAsFactors = FALSE,
               showProgress = FALSE)
glimpse(where9am)
```


## Computing probabilities

Find P(office) using `nrow()` and `subset()` to count rows in the dataset and save the result as `p_A`.

```{r p_A}
p_A <- nrow(subset(where9am, location == "office")) / nrow (where9am)
```

Find P(weekday), using `nrow()` and `subset()` again, and save the result as `p_B`.

```{r p_B}
p_B <- nrow(subset(where9am, daytype == "weekday")) / nrow (where9am)
```

Use `nrow()` and `subset()` a final time to find P(office and weekday). Save the result as `p_AB`.

```{r p_AB}
p_AB <- nrow(subset(where9am, location == "office" & daytype == "weekday")) / nrow (where9am)
```

Compute P(office | weekday) and save the result as `p_A_given_B`.

```{r p_A_given_B}
p_A_given_B <- p_AB / p_B
```

Print the value of p_A_given_B


```{r}
p_A_given_B
```

## A simple Naive Bayes location model

Use `naive_bayes()` with a formula like `y ~ x` to build a model of `location` as a function of `daytype`.

```{r model_1}
# Build the location prediction model
locmodel <- naive_bayes(location ~ daytype, data = where9am)
```

Forecast the Thursday 9am location using`predict()` with the `thursday9am `object as the `newdata` argument.

```{r thursday9am}
# Predict Thursday's 9am location
predict(locmodel, newdata = "weekday")
```

Do the same for predicting the `saturday9am` location.
```{r weekend}
predict(locmodel, newdata = "weekend")
```

## A more sophisticated location model
Use the R formula interface to build a model where location depends on both `daytype` and `hourtype`
```{r model_2}
# Build the location prediction model
locmodel <- naive_bayes(location ~ daytype + hourtype, data = where9am)
```

Forecast the Thursday 9am location using`predict()` with the `thursday9am `object as the `newdata` argument.

```{r weekday_afternoon}
# Predict Thursday's 9am location
predict(locmodel, newdata = c("weekday", "afternoon"))
```

Do the same for predicting the `saturday9am` location.
```{r weekday_evening}
predict(locmodel, newdata = c("weekday", "evening"))
```


## Preparing for unforeseen circumstances
``` {r}
predict(locmodel, newdata = c("weekend", "afternoon"), type = "prob")
```

```{r model_2}
# Build the location prediction model
locmodel <- naive_bayes(location ~ daytype + hourtype, data = where9am, laplace = 1)
```


``` {r}
predict(locmodel, newdata = c("weekend", "afternoon"), type = "prob")
```


# Logistic Regression

Logistic regression involves fitting a curve to numeric data to make predictions about binary events. Arguably one of the most widely used machine learning methods, this chapter will provide an overview of the technique while illustrating how to apply it to fundraising data.

```{r load_data_3}
donors <- fread("data/donors.csv", stringsAsFactors = TRUE,
               showProgress = FALSE)
glimpse(donors)
```

## Building simple logistic regression models
The `donors` dataset contains 93,462 examples of people mailed in a fundraising solicitation for paralyzed military veterans. The `donated` column is `1` if the person made a donation in response to the mailing and `0` otherwise. This binary outcome will be the dependent variable for the logistic regression model.

The remaining columns are features of the prospective donors that may influence their donation behavior. These are the model's independent variables.

When building a regression model, it it often helpful to form a hypothesis about which independent variables will be predictive of the dependent variable. The `bad_address` column, which is set to `1` for an invalid mailing address and `0` otherwise, seems like it might reduce the chances of a donation. Similarly, one might suspect that religious interest (`interest_religion`) and interest in veterans affairs (`interest_veterans`) would be associated with greater charitable giving.

In this exercise, you will use these three factors to create a simple model of donation behavior.

Count the number of occurrences of each level of the `donated` variable
```{r donated}
table(donors$donated)

# Build the donation model
donation_model <- glm(donated ~ bad_address + interest_religion + interest_veterans, 
                      data = donors, family = "binomial")

# Summarize the model results
summary(donation_model)
```


## Making a binary prediction

```{r b_model}

# Estimate the donation probability
donors$donation_prob <- predict(donation_model, type = "response")

# Find the donation probability of the average prospect
mean(donors$donated)

# Predict a donation if probability of donation is greater than average (0.0504)
donors$donation_pred <- ifelse(donors$donation_prob > 0.0504, 1, 0)

# Calculate the model's accuracy
mean(donors$donated == donors$donation_pred)
```

## Calculating ROC Curves and AUC

```{r auc_roc}
# Create a ROC curve
ROC <- roc(donors$donated, donors$donation_prob)

# Plot the ROC curve
plot(ROC, col = "blue")

# Calculate the area under the curve (AUC)
auc(ROC)
```

## Coding categorical features
Sometimes a dataset contains numeric values that represent a categorical feature.

In the `donors` dataset, `wealth_rating` uses numbers to indicate the donor's wealth level:

* 0 = Unknown
* 1 = Low
* 2 = Medium
* 3 = High

This exercise illustrates how to prepare this type of categorical feature and the examines its impact on a logistic regression model.

```{r fact_model}
# Convert the wealth rating to a factor
donors$wealth_rating <- factor(donors$wealth_rating,
levels = c(0, 1, 2, 3),
labels = c("Unknown", "Low", "Medium", "High"))

# Use relevel() to change reference category
donors$wealth_rating <- relevel(donors$wealth_rating, ref = "Medium")

# Build the donation model
donation_model <- glm(donated ~ wealth_rating, 
                      data = donors, family = "binomial")

# Summarize the model results
summary(donation_model)

# See how our factor coding impacts the model
summary(donation_model)
```

## Handling missing data
Some of the prospective donors have missing age data. Unfortunately, R will exclude any cases with NA values when building a regression model.

One workaround is to replace, or impute, the missing values with an estimated value. After doing so, you may also create a missing data indicator to model the possibility that cases with missing data are different in some way from those without.

```{r missing}
# Find the average age among non-missing values
summary(donors$age)

# Impute missing age values with mean(age)
donors$imputed_age <- ifelse(is.na(donors$age),
round(mean(donors$age, na.rm = TRUE), 2), donors$age)

# Create missing value indicator for age
donors$missing_age <- ifelse(is.na(donors$age),
1, 0)
```

## Building a more sophisticated model
One of the best predictors of future giving is a history of recent, frequent, and large gifts. In marketing terms, this is known as R/F/M:

Recency
Frequency
Money
Donors that haven given both recently and frequently may be especially likely to give again; in other words, the combined impact of recency and frequency may be greater than the sum of the separate effects.

Because these predictors together have a greater impact on the dependent variable, their joint effect must be modeled as an interaction.

```{r complex}
# Build a recency, frequency, and money (RFM) model
rfm_model <- glm(donated ~ money + recency * frequency, 
                      data = donors, family = "binomial")

# Summarize the RFM model to see how the parameters were coded
summary(rfm_model)

# Compute predicted probabilities for the RFM model
rfm_prob <- predict(rfm_model, type = "response")

# Plot the ROC curve and find AUC for the new model
ROC <- roc(donors$donated, rfm_prob)
plot(ROC, col = "red")
auc(ROC)
```

## Building a stepwise regression model
In the absence of subject-matter expertise, stepwise regression can assist with the search for the most important predictors of the outcome of interest.

In this exercise, you will use a forward stepwise approach to add predictors to the model one-by-one until no additional benefit is seen.

```{r step_wise}
# Specify a null model with no predictors
null_model <- glm(donated ~ 1, data = donors, family = "binomial")

# Specify the full model using all of the potential predictors
full_model <- glm(donated ~ ., data = donors, family = "binomial")

# Use a forward stepwise algorithm to build a parsimonious model
step_model <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")

# Estimate the stepwise donation probability
step_prob <- predict(step_model, type = "response")

# Plot the ROC of the stepwise model
ROC <- roc(donors$donated, step_prob)
plot(ROC, col = "red")
auc(ROC)

```



# Classification Trees
Classification trees use flowchart-like structures to make decisions. Because humans can readily understand these tree structures, classification trees are useful when transparency is needed, such as in loan approval. We'll use the Lending Club dataset to simulate this scenario.


```{r load_data_4}
loans <- fread("data/loans.csv", stringsAsFactors = TRUE,
               showProgress = FALSE)
glimpse(loans)
```

## Building a simple decision tree
The loans dataset contains 11,312 randomly-selected people who were applied for and later received loans from Lending Club, a US-based peer-to-peer lending company.

You will use a decision tree to try to learn patterns in the outcome of these loans (either repaid or default) based on the requested loan amount and credit score at the time of application.

Then, see how the tree's predictions differ for an applicant with good credit versus one with bad credit.

```{r first_tree}
# Build a lending model predicting loan outcome versus loan amount and credit score
loan_model <- rpart(outcome ~ loan_amount + credit_score, data = loans, method = "class", control = rpart.control(cp = 0))

# Make a prediction for someone with good credit
predict(loan_model, good_credit, type = "class")

# Make a prediction for someone with bad credit
predict(loan_model, bad_credit, type = "class")
```

## Visualizing classification trees
Due to government rules to prevent illegal discrimination, lenders are required to explain why a loan application was rejected.

The structure of classification trees can be depicted visually, which helps to understand how the tree makes its decisions.

```{r visu}
# Plot the loan_model with default settings
rpart.plot(loan_model)

# Plot the loan_model with customized settings
rpart.plot(loan_model, type = 3, box.palette = c("red", "green"), fallen.leaves = TRUE)
```

## Creating random test datasets
Before building a more sophisticated lending model, it is important to hold out a portion of the loan data to simulate how well it will predict the outcomes of future loan applicants.

As depicted in the following image, you can use 75% of the observations for training and 25% for testing the model.

The sample() function can be used to generate a random sample of rows to include in the training set. Simply supply it the total number of observations and the number needed for training.

Use the resulting vector of row IDs to subset the loans into training and testing datasets.

```{r split data}
# Determine the number of rows for training
nrow(loans) * 0.75

# Create a random sample of row IDs
sample_rows <- sample(11312, 8484)

# Create the training dataset
loans_train <- loans[sample_rows, ]

# Create the test dataset
loans_test <- loans[-sample_rows, ]
```

## Building and evaluating a larger tree
Previously, you created a simple decision tree that used the applicant's credit score and requested loan amount to predict the loan outcome.

Lending Club has additional information about the applicants, such as home ownership status, length of employment, loan purpose, and past bankruptcies, that may be useful for making more accurate predictions.

Using all of the available applicant data, build a more sophisticated lending model using the random training dataset created previously. Then, use this model to make predictions on the testing dataset to estimate the performance of the model on future loan applications.

```{r large_tree}
# Grow a tree using all of the available applicant data
loan_model <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0))

# Make predictions on the test dataset
loans_test$pred <- predict(loan_model, loans_test, type = "class")

# Examine the confusion matrix
table(loans_test$pred, loans_test$outcome)

# Compute the accuracy on the test dataset
mean(loans_test$pred == loans_test$outcome)
```

## Preventing overgrown trees
The tree grown on the full set of applicant data grew to be extremely large and extremely complex, with hundreds of splits and leaf nodes containing only a handful of applicants. This tree would be almost impossible for a loan officer to interpret.

Using the pre-pruning methods for early stopping, you can prevent a tree from growing too large and complex. See how the rpart control options for maximum tree depth and minimum split count impact the resulting tree.

```{r overfrow}
# Grow a tree with maxdepth of 6
loan_model <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0, maxdepth = 6))

# Compute the accuracy of the simpler tree
loans_test$pred <- predict(loan_model, loans_test, type = "class")
mean(loans_test$pred == loans_test$outcome)

# Grow a tree with minsplit of 500
loan_model2 <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0, minsplit = 500))

# Compute the accuracy of the simpler tree
loans_test$pred2 <- predict(loan_model2, loans_test, type = "class")
mean(loans_test$pred2 == loans_test$outcome)
```

## Creating a nicely pruned tree
Stopping a tree from growing all the way can lead it to ignore some aspects of the data or miss important trends it may have discovered later.

By using post-pruning, you can intentionally grow a large and complex then prune it to be smaller and more efficient later on.

In this exercise, you will have the opportunity to construct a visualization of the tree's performance versus complexity, and use this information to prune the tree to an appropriate level.

```{r prune}
# The 'rpart' package is loaded into the workspace

# Grow an overly complex tree
loan_model <- rpart(outcome ~ ., data = loans_train, method = "class", control = rpart.control(cp = 0))

# Examine the complexity plot
plotcp(loan_model)

# Prune the tree
loan_model_pruned <- prune(loan_model, cp = 0.0014)

# Compute the accuracy of the pruned tree
loans_test$pred <- predict(loan_model_pruned, loans_test, type = "class")
mean(loans_test$pred == loans_test$outcome)
```

## Building a random forest model
In spite of the fact that a forest can contain hundreds of trees, growing a decision tree forest is perhaps even easier than creating a single highly-tuned tree.

Using the randomForest package, build a random forest and see how it compares to the single trees you built previously.

Keep in mind that due to the random nature of the forest, the results may vary slightly each time you create the forest.

```{r random_forest}
# Build a random forest model
loan_model <- randomForest(outcome ~ ., data = loans_train)

# Compute the accuracy of the random forest
loans_test$pred <- predict(loan_model, loans_test, type = "class")
mean(loans_test$pred == loans_test$outcome)
```